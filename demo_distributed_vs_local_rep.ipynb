{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_kcs = 20\n",
    "pc_group = [0.3, 0.5, 0.8, 0.1]\n",
    "n_groups = len(pc_group)\n",
    "n_obs_per_kc = 50\n",
    "\n",
    "# random assignment matrix\n",
    "A = rng.multinomial(1, [1/n_groups] * n_groups, n_kcs)\n",
    "G = np.argmax(A, axis=1)\n",
    "print(A)\n",
    "x = []\n",
    "y = []\n",
    "for kc in np.arange(n_kcs):\n",
    "    g = G[kc]\n",
    "    pc = pc_group[g]\n",
    "    for i in range(n_obs_per_kc):\n",
    "        x.append(kc)\n",
    "        y.append(rng.binomial(1, pc))\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "X = keras.utils.to_categorical(x)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a simple membership model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:   1.0435\n",
      "Loss:   0.6481\n",
      "Loss:   0.6174\n",
      "Loss:   0.6011\n",
      "Loss:   0.5965\n",
      "Loss:   0.5852\n",
      "Loss:   0.5743\n",
      "Loss:   0.5666\n",
      "Loss:   0.5636\n",
      "Loss:   0.5625\n"
     ]
    }
   ],
   "source": [
    "# variable representing assignment probabilities\n",
    "logit_P = tf.Variable(tf.random.normal((n_kcs, n_groups), mean=0, stddev=0.1), name=\"logit_P\")\n",
    "\n",
    "# variable representing emission probabilities\n",
    "logit_O = tf.Variable(tf.random.normal((n_groups,1), mean=0, stddev=0.1), name=\"logit_O\")\n",
    "\n",
    "Xtf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "Ytf = tf.convert_to_tensor(y[:,np.newaxis], dtype=tf.float32)\n",
    "lossf =keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "\n",
    "epochs = 10000\n",
    "for e in range(epochs):\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        S = tf.math.exp(logit_P) / tf.reduce_sum(logit_P, axis=1, keepdims=True)\n",
    "        \n",
    "        # extract emission probabilities\n",
    "        logit_pc = tf.matmul(tf.matmul(X, S), logit_O)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = lossf(Ytf, logit_pc)\n",
    "    \n",
    "    trainables = [logit_P, logit_O]\n",
    "    grads = t.gradient(loss, trainables)\n",
    "    optimizer.apply_gradients(zip(grads, trainables))\n",
    "    \n",
    "    if e % 1000 == 0:\n",
    "        print(\"Loss: %8.4f\" % loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11367014, 0.66704434, 0.08698247, 0.132303  ],\n",
       "       [0.23458944, 0.31500798, 0.22713794, 0.22326466],\n",
       "       [0.1321291 , 0.6493795 , 0.08945897, 0.12903242],\n",
       "       [0.24811177, 0.2621024 , 0.26495832, 0.22482753],\n",
       "       [0.26800486, 0.2248898 , 0.25165382, 0.25545147],\n",
       "       [0.12813126, 0.6435085 , 0.10383385, 0.12452642],\n",
       "       [0.13496862, 0.63619095, 0.09894469, 0.12989576],\n",
       "       [0.1200204 , 0.6447525 , 0.09857304, 0.13665406],\n",
       "       [0.10909919, 0.6762711 , 0.08876687, 0.12586284],\n",
       "       [0.23212045, 0.3752901 , 0.17686096, 0.21572846],\n",
       "       [0.10063827, 0.69143814, 0.08350869, 0.12441489],\n",
       "       [0.29794988, 0.18661135, 0.24030066, 0.27513808],\n",
       "       [0.09852854, 0.7169448 , 0.0836041 , 0.10092262],\n",
       "       [0.14307392, 0.6012197 , 0.11503052, 0.14067586],\n",
       "       [0.22936614, 0.254871  , 0.25031465, 0.26544818],\n",
       "       [0.11603247, 0.6673513 , 0.08913586, 0.12748036],\n",
       "       [0.16001458, 0.5707206 , 0.11844661, 0.15081823],\n",
       "       [0.09201317, 0.7167462 , 0.0864086 , 0.10483195],\n",
       "       [0.1148174 , 0.6639429 , 0.09615196, 0.12508774],\n",
       "       [0.21544585, 0.36523044, 0.21275   , 0.20657372]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp = logit_P.numpy()\n",
    "lp = np.exp(lp) / np.sum(np.exp(lp), axis=1, keepdims=True)\n",
    "lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:   0.6969\n",
      "Loss:   0.5702\n",
      "Loss:   0.5712\n",
      "Loss:   0.5695\n",
      "Loss:   0.5708\n",
      "Loss:   0.5711\n",
      "Loss:   0.5724\n",
      "Loss:   0.5716\n",
      "Loss:   0.5699\n",
      "Loss:   0.5699\n",
      "tf.Tensor(\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]], shape=(20, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# variable representing assignment probabilities\n",
    "logit_P = tf.Variable(tf.random.normal((n_kcs, n_groups), mean=0, stddev=0.1), name=\"logit_P\")\n",
    "\n",
    "# variable representing emission probabilities\n",
    "logit_O = tf.Variable(tf.random.normal((n_groups,1), mean=0, stddev=0.1), name=\"logit_O\")\n",
    "\n",
    "Xtf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "Ytf = tf.convert_to_tensor(y[:,np.newaxis], dtype=tf.float32)\n",
    "lossf =keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "temperature = 0.5\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.1)\n",
    "\n",
    "epochs = 10000\n",
    "for e in range(epochs):\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        dist = tfp.distributions.RelaxedOneHotCategorical(temperature, logits=logit_P)\n",
    "        \n",
    "        # sample an assignment [n_kcs, n_groups]\n",
    "        S = dist.sample()\n",
    "        \n",
    "        # quantize it\n",
    "        #S = quantize(S)\n",
    "        \n",
    "        # extract emission probabilities\n",
    "        logit_pc = tf.matmul(tf.matmul(X, S), logit_O)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = lossf(Ytf, logit_pc)\n",
    "    \n",
    "    trainables = [logit_P, logit_O]\n",
    "    grads = t.gradient(loss, trainables)\n",
    "    optimizer.apply_gradients(zip(grads, trainables))\n",
    "    \n",
    "    if e % 1000 == 0:\n",
    "        print(\"Loss: %8.4f\" % loss.numpy())\n",
    "\n",
    "dist = tfp.distributions.RelaxedOneHotCategorical(1e-6, logits=logit_P)\n",
    "# sample an assignment [n_kcs, n_groups]\n",
    "S = dist.sample()\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.75, 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.43, 0.57, 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.3 , 0.7 , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "for i in range(100):\n",
    "    samples.append(dist.sample().numpy())\n",
    "samples = np.array(samples)\n",
    "np.mean(samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
